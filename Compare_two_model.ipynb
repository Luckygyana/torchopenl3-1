{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compare two model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqgeGAGSY3dY",
        "outputId": "d9e8db35-4a71-4b08-bce8-63f129f99344"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JfgnnjbY5T1"
      },
      "source": [
        "!pip install openl3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-yjwn3WZYLf",
        "outputId": "55901672-355d-47bc-8f46-eac450cac642"
      },
      "source": [
        "import openl3\r\n",
        "from keras import Model, Input\r\n",
        "\r\n",
        "import librosa\r\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujcRgNVCY8TV"
      },
      "source": [
        "model = openl3.models.load_audio_embedding_model(input_repr=\"mel128\", content_type=\"env\", embedding_size=6144)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3jZhCs7ZGpO"
      },
      "source": [
        "inp = model.get_layer('input_1').input\r\n",
        "oups = [\r\n",
        "    model.get_layer('melspectrogram_1').output,\r\n",
        "    model.get_layer('batch_normalization_1').output,\r\n",
        "    model.get_layer('conv2d_1').output,\r\n",
        "    model.get_layer('batch_normalization_2').output,\r\n",
        "    model.get_layer('activation_1').output,\r\n",
        "    model.get_layer('conv2d_2').output,\r\n",
        "    model.get_layer('batch_normalization_3').output,\r\n",
        "    model.get_layer('activation_2').output,\r\n",
        "    model.get_layer('conv2d_3').output,\r\n",
        "    model.get_layer('batch_normalization_4').output,\r\n",
        "    model.get_layer('batch_normalization_8').output,\r\n",
        "    model.get_layer('activation_7').output,\r\n",
        "    model.get_layer('audio_embedding_layer').output,\r\n",
        "    model.get_layer('max_pooling2d_4').output,\r\n",
        "    model.get_layer('flatten_1').output,\r\n",
        "]\r\n",
        "\r\n",
        "model2 = Model(inputs=[inp], outputs=oups)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BHGy3pNZdMF"
      },
      "source": [
        "audio_np, sr = librosa.load(\r\n",
        "        '/content/drive/MyDrive/Audiomodel/sampleaudio.623702.ogg',\r\n",
        "        sr=48000) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsIecdgNZdOc",
        "outputId": "656ac53a-f6b0-4696-8a2e-26df2f190242"
      },
      "source": [
        "TARGET_SR = 48000\r\n",
        "hop_size=0.1\r\n",
        "hop_len = int(hop_size * TARGET_SR)\r\n",
        "frame_len = TARGET_SR\r\n",
        "def _pad_audio(audio, frame_len, hop_len):\r\n",
        "    \"\"\"Pad audio if necessary so that all samples are processed\"\"\"\r\n",
        "    audio_len = audio.size\r\n",
        "    if audio_len < frame_len:\r\n",
        "        pad_length = frame_len - audio_len\r\n",
        "    else:\r\n",
        "        pad_length = int(np.ceil((audio_len - frame_len)/float(hop_len))) * hop_len \\\r\n",
        "                     - (audio_len - frame_len)\r\n",
        "\r\n",
        "    if pad_length > 0:\r\n",
        "        audio = np.pad(audio, (0, pad_length), mode='constant', constant_values=0)\r\n",
        "\r\n",
        "    return audio\r\n",
        "\r\n",
        "audio = _pad_audio(audio_np, frame_len, hop_len)\r\n",
        "\r\n",
        "n_frames = 1 + int((len(audio) - frame_len) / float(hop_len))\r\n",
        "x = np.lib.stride_tricks.as_strided(audio, shape=(frame_len, n_frames),\r\n",
        "        strides=(audio.itemsize, hop_len * audio.itemsize)).T\r\n",
        "x = x.reshape((x.shape[0], 1, x.shape[-1]))\r\n",
        "print(x.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 1, 48000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EyaxRPnZReP",
        "outputId": "8f521e50-362f-4582-c7a2-8381ff30a85e"
      },
      "source": [
        "tensorflow_ouputs = model2.predict(x, verbose=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r32/32 [==============================] - 11s 347ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvkPOSFLZs-d"
      },
      "source": [
        "mel_spec_output = tensorflow_ouputs[0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m95mv4ZJZtB1"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import math"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxW7jR24ZtE4"
      },
      "source": [
        "def load_weights(weight_file):\r\n",
        "    if weight_file == None:\r\n",
        "        return\r\n",
        "\r\n",
        "    try:\r\n",
        "        weights_dict = np.load(weight_file,allow_pickle=True).item()\r\n",
        "    except:\r\n",
        "        weights_dict = np.load(weight_file, encoding='bytes',allow_pickle=True).item()\r\n",
        "\r\n",
        "    return weights_dict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "345cXy6FZtHM"
      },
      "source": [
        "class PytorchOpenl3(nn.Module):\r\n",
        "\r\n",
        "    \r\n",
        "    def __init__(self, weight_file):\r\n",
        "        super(PytorchOpenl3, self).__init__()\r\n",
        "        self.__weights_dict = load_weights(weight_file)\r\n",
        "\r\n",
        "        self.batch_normalization_1 = self.__batch_normalization(2, 'batch_normalization_1', num_features=1, eps=0.001, momentum=0.99)\r\n",
        "        self.conv2d_1 = self.__conv(2, name='conv2d_1', in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "        self.batch_normalization_2 = self.__batch_normalization(2, 'batch_normalization_2', num_features=64, eps=0.001, momentum=0.99)\r\n",
        "        self.conv2d_2 = self.__conv(2, name='conv2d_2', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "        self.batch_normalization_3 = self.__batch_normalization(2, 'batch_normalization_3', num_features=64, eps=0.001, momentum=0.99)\r\n",
        "        self.conv2d_3 = self.__conv(2, name='conv2d_3', in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "        self.batch_normalization_4 = self.__batch_normalization(2, 'batch_normalization_4', num_features=128, eps=0.001, momentum=0.99)\r\n",
        "        self.conv2d_4 = self.__conv(2, name='conv2d_4', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "        self.batch_normalization_5 = self.__batch_normalization(2, 'batch_normalization_5', num_features=128, eps=0.001, momentum=0.99)\r\n",
        "        self.conv2d_5 = self.__conv(2, name='conv2d_5', in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "        self.batch_normalization_6 = self.__batch_normalization(2, 'batch_normalization_6', num_features=256, eps=0.001, momentum=0.99)\r\n",
        "        self.conv2d_6 = self.__conv(2, name='conv2d_6', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "        self.batch_normalization_7 = self.__batch_normalization(2, 'batch_normalization_7', num_features=256, eps=0.001, momentum=0.99)\r\n",
        "        self.conv2d_7 = self.__conv(2, name='conv2d_7', in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "        self.batch_normalization_8 = self.__batch_normalization(2, 'batch_normalization_8', num_features=512, eps=0.001, momentum=0.99)\r\n",
        "        self.audio_embedding_layer = self.__conv(2, name='audio_embedding_layer', in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        batch_normalization_1 = self.batch_normalization_1(x)\r\n",
        "        conv2d_1_pad    = F.pad(batch_normalization_1, (1, 1, 1, 1))\r\n",
        "        conv2d_1        = self.conv2d_1(conv2d_1_pad)\r\n",
        "        batch_normalization_2 = self.batch_normalization_2(conv2d_1)\r\n",
        "        activation_1    = F.relu(batch_normalization_2)\r\n",
        "        conv2d_2_pad    = F.pad(activation_1, (1, 1, 1, 1))\r\n",
        "        conv2d_2        = self.conv2d_2(conv2d_2_pad)\r\n",
        "        batch_normalization_3 = self.batch_normalization_3(conv2d_2)\r\n",
        "        activation_2    = F.relu(batch_normalization_3)\r\n",
        "        max_pooling2d_1 = F.max_pool2d(activation_2, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\r\n",
        "        conv2d_3_pad    = F.pad(max_pooling2d_1, (1, 1, 1, 1))\r\n",
        "        conv2d_3        = self.conv2d_3(conv2d_3_pad)\r\n",
        "        batch_normalization_4 = self.batch_normalization_4(conv2d_3)\r\n",
        "        activation_3    = F.relu(batch_normalization_4)\r\n",
        "        conv2d_4_pad    = F.pad(activation_3, (1, 1, 1, 1))\r\n",
        "        conv2d_4        = self.conv2d_4(conv2d_4_pad)\r\n",
        "        batch_normalization_5 = self.batch_normalization_5(conv2d_4)\r\n",
        "        activation_4    = F.relu(batch_normalization_5)\r\n",
        "        max_pooling2d_2 = F.max_pool2d(activation_4, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\r\n",
        "        conv2d_5_pad    = F.pad(max_pooling2d_2, (1, 1, 1, 1))\r\n",
        "        conv2d_5        = self.conv2d_5(conv2d_5_pad)\r\n",
        "        batch_normalization_6 = self.batch_normalization_6(conv2d_5)\r\n",
        "        activation_5    = F.relu(batch_normalization_6)\r\n",
        "        conv2d_6_pad    = F.pad(activation_5, (1, 1, 1, 1))\r\n",
        "        conv2d_6        = self.conv2d_6(conv2d_6_pad)\r\n",
        "        batch_normalization_7 = self.batch_normalization_7(conv2d_6)\r\n",
        "        activation_6    = F.relu(batch_normalization_7)\r\n",
        "        max_pooling2d_3 = F.max_pool2d(activation_6, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\r\n",
        "        conv2d_7_pad    = F.pad(max_pooling2d_3, (1, 1, 1, 1))\r\n",
        "        conv2d_7        = self.conv2d_7(conv2d_7_pad)\r\n",
        "        batch_normalization_8 = self.batch_normalization_8(conv2d_7)\r\n",
        "        activation_7    = F.relu(batch_normalization_8)\r\n",
        "        audio_embedding_layer_pad = F.pad(activation_7, (1, 1, 1, 1))\r\n",
        "        audio_embedding_layer = self.audio_embedding_layer(audio_embedding_layer_pad)\r\n",
        "        max_pooling2d_4 = F.max_pool2d(audio_embedding_layer, kernel_size=(4, 8), stride=(4, 8), padding=0, ceil_mode=False)\r\n",
        "        return max_pooling2d_4\r\n",
        "\r\n",
        "\r\n",
        "    def __batch_normalization(self, dim, name, **kwargs):\r\n",
        "        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)\r\n",
        "        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)\r\n",
        "        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)\r\n",
        "        else:           raise NotImplementedError()\r\n",
        "\r\n",
        "        if 'scale' in self.__weights_dict[name]:\r\n",
        "            layer.state_dict()['weight'].copy_(torch.from_numpy(self.__weights_dict[name]['scale']))\r\n",
        "        else:\r\n",
        "            layer.weight.data.fill_(1)\r\n",
        "\r\n",
        "        if 'bias' in self.__weights_dict[name]:\r\n",
        "            layer.state_dict()['bias'].copy_(torch.from_numpy(self.__weights_dict[name]['bias']))\r\n",
        "        else:\r\n",
        "            layer.bias.data.fill_(0)\r\n",
        "\r\n",
        "        layer.state_dict()['running_mean'].copy_(torch.from_numpy(self.__weights_dict[name]['mean']))\r\n",
        "        layer.state_dict()['running_var'].copy_(torch.from_numpy(self.__weights_dict[name]['var']))\r\n",
        "        return layer\r\n",
        "\r\n",
        "    def __conv(self, dim, name, **kwargs):\r\n",
        "        if   dim == 1:  layer = nn.Conv1d(**kwargs)\r\n",
        "        elif dim == 2:  layer = nn.Conv2d(**kwargs)\r\n",
        "        elif dim == 3:  layer = nn.Conv3d(**kwargs)\r\n",
        "        else:           raise NotImplementedError()\r\n",
        "\r\n",
        "        layer.state_dict()['weight'].copy_(torch.from_numpy(self.__weights_dict[name]['weights']))\r\n",
        "        if 'bias' in self.__weights_dict[name]:\r\n",
        "            layer.state_dict()['bias'].copy_(torch.from_numpy(self.__weights_dict[name]['bias']))\r\n",
        "        return layer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1oNZRDNZ7v_"
      },
      "source": [
        "model = PytorchOpenl3('/content/drive/MyDrive/Audiomodel/openl3_no_mel_layer_pytorch_weights')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuxeke5zZ_xL"
      },
      "source": [
        "with torch.set_grad_enabled(False):\r\n",
        "    model = model.eval()\r\n",
        "    pytorch_output = model(\r\n",
        "        torch.Tensor(mel_spec_output.swapaxes(2, 3).swapaxes(1, 2))\r\n",
        "    )\r\n",
        "    pytorch_output = pytorch_output.detach().numpy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVQIZP1qZ_zu"
      },
      "source": [
        "tensorflow_ouput = tensorflow_ouputs[-2].swapaxes(2, 3).swapaxes(1, 2)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmnHOafMdRbj",
        "outputId": "043ea361-62d8-4e60-d234-897a57b669f3"
      },
      "source": [
        "tensorflow_ouput.shape,pytorch_output.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 512, 4, 3), (32, 512, 4, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "reAXBlDRdXy8",
        "outputId": "09d8aec9-7d4d-44db-aebd-a0832d3c52ce"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "plt.scatter(tensorflow_ouput[0, :, 0, 0], pytorch_output[0, :, 0, 0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f9e5c1d2190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV6dlbEsdhqG",
        "outputId": "02d7cc6a-2162-4423-f395-a5ab8fe13a8a"
      },
      "source": [
        "np.mean(np.abs(tensorflow_ouput.reshape(32,-1) - pytorch_output.reshape(32,-1)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.2225636e-07"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}